{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "183ce48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import jiwer\n",
    "from tqdm import tqdm\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\"\n",
    "os.environ[\"PYTORCH_ENABLE_MPS_FALLBACK\"] = \"1\"\n",
    "import math\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "from dataset import get_dataset, get_tokenizer\n",
    "from transcribe_model import TranscribeModel\n",
    "from torch import nn\n",
    "\n",
    "vq_initial_loss_weight = 0.01 \n",
    "vq_warmup_steps = 1000\n",
    "vq_final_loss_weight = 0.001\n",
    "num_epochs = 1000\n",
    "starting_steps = 0\n",
    "num_examples = None\n",
    "model_id = \"test21\"\n",
    "num_batch_repeats = 1\n",
    "\n",
    "starting_steps = 0\n",
    "BATCH_SIZE = 64\n",
    "LEARNING_RATE = 1e-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53cbda43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_loss_function(log_probs, target, blank_token):\n",
    "    loss_function = nn.CTCLoss(blank=blank_token)\n",
    "    \n",
    "    input_lengths = torch.full((log_probs.shape[0],), log_probs.shape[1], \n",
    "                              dtype=torch.long, device=log_probs.device)\n",
    "    \n",
    "    # Use torch.ne for element-wise comparison\n",
    "    target_lengths = torch.ne(target, blank_token).sum(dim=1).to(torch.long)\n",
    "    \n",
    "    input_seq_first = log_probs.permute(1, 0, 2)\n",
    "    loss = loss_function(input_seq_first, target, input_lengths, target_lengths)\n",
    "    return loss\n",
    "\n",
    "def safe_mean(losses):\n",
    "    \"\"\"Calculate mean safely, handling empty lists\"\"\"\n",
    "    return sum(losses) / len(losses) if len(losses) > 0 else 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30f76017",
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decoder(log_probs, blank_token=0):\n",
    "    \"\"\"Improved greedy decoder for CTC outputs.\"\"\"\n",
    "    # Get the most likely token at each timestep\n",
    "    predictions = torch.argmax(log_probs, dim=-1).cpu().numpy()\n",
    "    decoded_predictions = []\n",
    "    \n",
    "    for pred in predictions:\n",
    "        # Remove consecutive duplicates and blanks\n",
    "        previous = -1\n",
    "        decoded_seq = []\n",
    "        for p in pred:\n",
    "            if p != previous and p != blank_token:\n",
    "                decoded_seq.append(p)\n",
    "            previous = p\n",
    "        decoded_predictions.append(decoded_seq)\n",
    "    \n",
    "    return decoded_predictions\n",
    "\n",
    "def calculate_wer(predictions, references):\n",
    "    \"\"\"Calculate Word Error Rate between predictions and references.\"\"\"\n",
    "    try:\n",
    "        return jiwer.wer(references, predictions)\n",
    "    except:\n",
    "        return 1.0\n",
    "    \n",
    "def calculate_cer(predictions, references):\n",
    "    \"\"\"Calculate Character Error Rate between predictions and references.\"\"\"\n",
    "    total_chars = sum(len(ref) for ref in references)\n",
    "    total_edits = 0\n",
    "    \n",
    "    for pred, ref in zip(predictions, references):\n",
    "        # Simple Levenshtein distance calculation\n",
    "        dp = [[0] * (len(ref) + 1) for _ in range(len(pred) + 1)]\n",
    "        \n",
    "        for i in range(len(pred) + 1):\n",
    "            dp[i][0] = i\n",
    "        for j in range(len(ref) + 1):\n",
    "            dp[0][j] = j\n",
    "            \n",
    "        for i in range(1, len(pred) + 1):\n",
    "            for j in range(1, len(ref) + 1):\n",
    "                if pred[i-1] == ref[j-1]:\n",
    "                    dp[i][j] = dp[i-1][j-1]\n",
    "                else:\n",
    "                    dp[i][j] = min(dp[i-1][j], dp[i][j-1], dp[i-1][j-1]) + 1\n",
    "        \n",
    "        total_edits += dp[len(pred)][len(ref)]\n",
    "    \n",
    "    return total_edits / total_chars if total_chars > 0 else 1.0\n",
    "\n",
    "def evaluate_model(model, dataloader, tokenizer, device, blank_token, max_batches=5):\n",
    "    \"\"\"Evaluate the model and return metrics with sample predictions.\"\"\"\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_references = []\n",
    "    sample_examples = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(dataloader):\n",
    "            if i >= max_batches:\n",
    "                break\n",
    "                \n",
    "            audio = batch[\"audio\"].to(device)\n",
    "            target = batch[\"input_ids\"].to(device)\n",
    "            text = batch[\"text\"]\n",
    "            \n",
    "            # Forward pass\n",
    "            if audio.dim() == 2:\n",
    "                audio = audio.unsqueeze(1)\n",
    "            \n",
    "            output, _ = model(audio)\n",
    "            blank_token_id = blank_token\n",
    "            # Decode predictions - use the correct blank token\n",
    "            decoded_preds = greedy_decoder(output, blank_token=blank_token)\n",
    "            \n",
    "            # Convert token IDs to text - FIXED VERSION\n",
    "            pred_texts = []\n",
    "            for pred in decoded_preds:\n",
    "                tokens = []\n",
    "                for p in pred:\n",
    "                    if p < len(tokenizer.get_vocab()) and p != blank_token_id:\n",
    "                        token = tokenizer.id_to_token(p)\n",
    "                        # Filter out ALL special tokens\n",
    "                        if token and token not in [\"<pad>\", \"<unk>\", \"<s>\", \"</s>\", \"<□>\"]:\n",
    "                            tokens.append(token)\n",
    "                pred_text = \"\".join(tokens)\n",
    "                pred_texts.append(pred_text)\n",
    "                \n",
    "            all_predictions.extend(pred_texts)     \n",
    "            all_references.extend(text) \n",
    "            # Store first few examples for display\n",
    "            if i < 3:\n",
    "                for j, (pred, ref) in enumerate(zip(pred_texts, text)):\n",
    "                    if len(sample_examples) < 6:\n",
    "                        sample_examples.append({\n",
    "                            'reference': ref,\n",
    "                            'prediction': pred,\n",
    "                            'batch': i,\n",
    "                            'sample': j\n",
    "                        })\n",
    "    \n",
    "    model.train()\n",
    "    \n",
    "    # Calculate metrics\n",
    "    wer = calculate_wer(all_predictions, all_references)\n",
    "    cer = calculate_cer(all_predictions, all_references)\n",
    "    \n",
    "    return {\n",
    "        'wer': wer,\n",
    "        'cer': cer,\n",
    "        'num_samples': len(all_predictions),\n",
    "        'examples': sample_examples\n",
    "    }\n",
    "    \n",
    "def print_evaluation_results(eval_results, step):\n",
    "    \"\"\"Print evaluation results in a nice format.\"\"\"\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(f\"EVALUATION RESULTS AT STEP {step}\")\n",
    "    print(\"=\"*80)\n",
    "    print(f\"Word Error Rate (WER): {eval_results['wer']:.4f}\")\n",
    "    print(f\"Character Error Rate (CER): {eval_results['cer']:.4f}\")\n",
    "    print(f\"Number of samples evaluated: {eval_results['num_samples']}\")\n",
    "    print(\"\\nSAMPLE PREDICTIONS:\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for i, example in enumerate(eval_results['examples']):\n",
    "        print(f\"\\nExample {i+1}:\")\n",
    "        print(f\"Reference:  '{example['reference']}'\")\n",
    "        print(f\"Prediction: '{example['prediction']}'\")\n",
    "        \n",
    "        # Calculate individual WER for this example\n",
    "        individual_wer = calculate_wer([example['prediction']], [example['reference']])\n",
    "        print(f\"Individual WER: {individual_wer:.4f}\")\n",
    "    \n",
    "    print(\"=\"*80 + \"\\n\")    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "38669b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    log_dir = f\"runs/speech2text_training/{model_id}\"\n",
    "    if os.path.exists(log_dir):\n",
    "        import shutil\n",
    "        shutil.rmtree(log_dir)\n",
    "    writer = SummaryWriter(log_dir)\n",
    "\n",
    "    tokenizer = get_tokenizer()\n",
    "    blank_token = tokenizer.token_to_id(\"<□>\")\n",
    "\n",
    "    device = torch.device(\n",
    "        \"cuda\"\n",
    "        if torch.cuda.is_available()\n",
    "        else \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "    )\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load or create model\n",
    "    if os.path.exists(r\"C:\\Users\\Kamil\\Desktop\\Coding\\models\\test21\\model_step_3000.pth\"):\n",
    "        print(f\"Loading model from models/{model_id}/model_step_3000.pth\")\n",
    "        model = TranscribeModel.load(r\"C:\\Users\\Kamil\\Desktop\\Coding\\models\\test21\\model_step_3000.pth\").to(device)\n",
    "    else:\n",
    "        model = TranscribeModel(\n",
    "            num_codebooks=4,        # Zwiększ z 2\n",
    "            codebook_size=64,       # Zwiększ z 32\n",
    "            embedding_dim=256,      # Zwiększ z 128\n",
    "            num_transformer_layers=6, # Zwiększ z 3\n",
    "            vocab_size=len(tokenizer.get_vocab()),  # DODANE - wymagane\n",
    "            strides=[8, 8, 4],      # Bardziej agresywne downsampling\n",
    "            initial_mean_pooling_kernel_size=2,     # DODANE - wymagane\n",
    "            max_seq_length=400,     # Zmniejsz dla pamięci\n",
    "        ).to(device)\n",
    "\n",
    "    num_trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f\"Number of trainable parameters: {num_trainable_params}\")\n",
    "\n",
    "    optimizer = torch.optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=LEARNING_RATE,\n",
    "        weight_decay=0.01,\n",
    "        betas=(0.9, 0.98)\n",
    "    )\n",
    "\n",
    "    dataloader = get_dataset(\n",
    "        batch_size=BATCH_SIZE,\n",
    "        num_examples=num_examples,\n",
    "        num_workers=0,\n",
    "    )\n",
    "\n",
    "    scheduler = OneCycleLR(\n",
    "        optimizer,\n",
    "        max_lr=LEARNING_RATE,\n",
    "        steps_per_epoch=len(dataloader),\n",
    "        epochs=num_epochs,\n",
    "        pct_start=0.1\n",
    "    )\n",
    "\n",
    "    # Create evaluation dataloader (smaller batch size for evaluation)\n",
    "    eval_dataloader = get_dataset(\n",
    "        batch_size=16,\n",
    "        num_examples=100,  # Evaluate on 100 samples\n",
    "        num_workers=0,\n",
    "    )\n",
    "\n",
    "    # Training configuration\n",
    "    vq_initial_loss_weight = 0.1\n",
    "    vq_warmup_steps = 2000\n",
    "    vq_final_loss_weight = 0.01\n",
    "    gradient_accumulation_steps = 4\n",
    "    \n",
    "    # Initialize loss tracking\n",
    "    ctc_losses = []\n",
    "    vq_losses = []\n",
    "    steps = starting_steps\n",
    "    \n",
    "    # Create directory for saving models\n",
    "    os.makedirs(f\"models/{model_id}\", exist_ok=True)\n",
    "\n",
    "    print(\"Starting training...\")\n",
    "    print(f\"Total steps per epoch: {len(dataloader)}\")\n",
    "    print(f\"Evaluation every 1000 steps\")\n",
    "    \n",
    "    for i in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_start_time = torch.cuda.Event(enable_timing=True) if torch.cuda.is_available() else None\n",
    "        epoch_end_time = torch.cuda.Event(enable_timing=True) if torch.cuda.is_available() else None\n",
    "        \n",
    "        if epoch_start_time:\n",
    "            epoch_start_time.record()\n",
    "        \n",
    "        for idx, batch in enumerate(dataloader):\n",
    "            audio = batch[\"audio\"].to(device)\n",
    "            target = batch[\"input_ids\"].to(device)\n",
    "\n",
    "            if audio.dim() == 2:\n",
    "                audio = audio.unsqueeze(1)\n",
    "\n",
    "            output, vq_loss = model(audio)\n",
    "            ctc_loss = run_loss_function(output, target, blank_token)\n",
    "\n",
    "            # Improved loss weighting with cosine annealing\n",
    "            progress = steps / (num_epochs * len(dataloader))\n",
    "            vq_weight = vq_final_loss_weight + (vq_initial_loss_weight - vq_final_loss_weight) * \\\n",
    "                       (1 + math.cos(math.pi * min(progress * 2, 1))) / 2\n",
    "\n",
    "            if vq_loss is not None:\n",
    "                total_loss = ctc_loss + vq_weight * vq_loss\n",
    "            else:\n",
    "                total_loss = ctc_loss\n",
    "\n",
    "            total_loss = total_loss / gradient_accumulation_steps\n",
    "            total_loss.backward()\n",
    "\n",
    "            ctc_losses.append(ctc_loss.item())\n",
    "            vq_losses.append(vq_loss.item() if vq_loss is not None else 0.0)\n",
    "\n",
    "            if (idx + 1) % gradient_accumulation_steps == 0 or (idx + 1) == len(dataloader):\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "            # Regular logging every 20 steps\n",
    "            if steps % 20 == 0:\n",
    "                if len(ctc_losses) > 0:\n",
    "                    avg_ctc_loss = safe_mean(ctc_losses)\n",
    "                    avg_vq_loss = safe_mean(vq_losses)\n",
    "                    avg_loss = avg_ctc_loss + vq_weight * avg_vq_loss\n",
    "\n",
    "                    print(\n",
    "                        f\"Epoch {i}, Batch {idx}/{len(dataloader)}, Step {steps}, \"\n",
    "                        f\"Loss: {avg_loss:.4f}, CTC Loss: {avg_ctc_loss:.4f}, \"\n",
    "                        f\"VQ Loss: {avg_vq_loss:.4f}, VQ Weight: {vq_weight:.4f}\"\n",
    "                    )\n",
    "\n",
    "                    writer.add_scalar(\"Loss/train\", avg_loss, steps)\n",
    "                    writer.add_scalar(\"Loss/ctc\", avg_ctc_loss, steps)\n",
    "                    writer.add_scalar(\"Loss/vq\", avg_vq_loss, steps)\n",
    "                    writer.add_scalar(\"Loss/vq_weight\", vq_weight, steps)\n",
    "\n",
    "                    ctc_losses = []\n",
    "                    vq_losses = []\n",
    "\n",
    "            # Evaluation every 1000 steps\n",
    "            if steps % 1000 == 0:\n",
    "                print(f\"\\nRunning evaluation at step {steps}...\")\n",
    "                eval_results = evaluate_model(model, eval_dataloader, tokenizer, device, blank_token)\n",
    "                \n",
    "                # Log metrics to tensorboard\n",
    "                writer.add_scalar(\"Metrics/WER\", eval_results['wer'], steps)\n",
    "                writer.add_scalar(\"Metrics/CER\", eval_results['cer'], steps)\n",
    "                \n",
    "                # Print detailed results\n",
    "                print_evaluation_results(eval_results, steps)\n",
    "\n",
    "            # Save model periodically\n",
    "            if steps % 500 == 0:\n",
    "                model_path = f\"models/{model_id}/model_step_{steps}.pth\"\n",
    "                os.makedirs(os.path.dirname(model_path), exist_ok=True)\n",
    "\n",
    "                try:\n",
    "                    torch.save({\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'step': steps,\n",
    "                        'epoch': i,\n",
    "                        'vq_weight': vq_weight,\n",
    "                    }, model_path)\n",
    "                    print(f\"Model saved to {model_path}\")\n",
    "\n",
    "                    latest_path = f\"models/{model_id}/model_latest.pth\"\n",
    "                    torch.save({\n",
    "                        'model_state_dict': model.state_dict(),\n",
    "                        'optimizer_state_dict': optimizer.state_dict(),\n",
    "                        'step': steps,\n",
    "                        'epoch': i,\n",
    "                        'vq_weight': vq_weight,\n",
    "                    }, latest_path)\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f\"Error saving model: {e}\")\n",
    "\n",
    "        if epoch_end_time:\n",
    "            epoch_end_time.record()\n",
    "            torch.cuda.synchronize()\n",
    "            epoch_time = epoch_start_time.elapsed_time(epoch_end_time) / 1000.0\n",
    "            print(f\"Epoch {i} completed in {epoch_time:.2f} seconds\")\n",
    "\n",
    "    # Final evaluation\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"FINAL EVALUATION\")\n",
    "    print(\"=\"*80)\n",
    "    final_eval_results = evaluate_model(model, eval_dataloader, tokenizer, device, blank_token, max_batches=10)\n",
    "    print_evaluation_results(final_eval_results, steps)\n",
    "\n",
    "    # Save final model\n",
    "    try:\n",
    "        final_path = f\"models/{model_id}/model_final.pth\"\n",
    "        torch.save({\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'step': steps,\n",
    "            'epoch': num_epochs,\n",
    "            'final_wer': final_eval_results['wer'],\n",
    "            'final_cer': final_eval_results['cer'],\n",
    "        }, final_path)\n",
    "        print(f\"Final model saved to {final_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving final model: {e}\")\n",
    "\n",
    "    writer.close()\n",
    "    print(\"Training completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "09a5f562",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokens: ['<pad>', '<unk>', 'H', 'E', 'L', 'L', 'O', '<□>']\n",
      "Filtered tokens: ['H', 'E', 'L', 'L', 'O']\n",
      "Expected: ['H', 'E', 'L', 'L', 'O']\n",
      "✅ Token filtering is working correctly!\n"
     ]
    }
   ],
   "source": [
    "# Add to train.py to test the tokenizer fix:\n",
    "def test_tokenizer_filtering():\n",
    "    tokenizer = get_tokenizer()\n",
    "    \n",
    "    # Test the filtering logic\n",
    "    test_tokens = [\"<pad>\", \"<unk>\", \"H\", \"E\", \"L\", \"L\", \"O\", \"<□>\"]\n",
    "    filtered = []\n",
    "    \n",
    "    for token in test_tokens:\n",
    "        if token and token not in [\"<pad>\", \"<unk>\", \"<s>\", \"</s>\", \"<□>\"]:\n",
    "            filtered.append(token)\n",
    "    \n",
    "    print(f\"Original tokens: {test_tokens}\")\n",
    "    print(f\"Filtered tokens: {filtered}\")\n",
    "    print(f\"Expected: ['H', 'E', 'L', 'L', 'O']\")\n",
    "    \n",
    "    if filtered == ['H', 'E', 'L', 'L', 'O']:\n",
    "        print(\"✅ Token filtering is working correctly!\")\n",
    "    else:\n",
    "        print(\"❌ Token filtering still has issues\")\n",
    "\n",
    "# Call this before training starts\n",
    "test_tokenizer_filtering()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "068673a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Number of trainable parameters: 2877489\n",
      "Starting training...\n",
      "Total steps per epoch: 21\n",
      "Evaluation every 1000 steps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Kamil\\Desktop\\Coding\\HuggingFace\\Agent\\lib\\site-packages\\torch\\nn\\modules\\conv.py:370: UserWarning: Using padding='same' with even kernel lengths and odd dilation may require a zero-padded copy of the input be created (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\pytorch\\aten\\src\\ATen\\native\\Convolution.cpp:1037.)\n",
      "  return F.conv1d(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Batch 19/21, Step 20, Loss: 6.9521, CTC Loss: 6.6293, VQ Loss: 3.2285, VQ Weight: 0.1000\n",
      "Epoch 1, Batch 18/21, Step 40, Loss: 6.9502, CTC Loss: 6.6266, VQ Loss: 3.2353, VQ Weight: 0.1000\n",
      "Epoch 2, Batch 17/21, Step 60, Loss: 6.9208, CTC Loss: 6.5959, VQ Loss: 3.2488, VQ Weight: 0.1000\n",
      "Epoch 3, Batch 16/21, Step 80, Loss: 6.9807, CTC Loss: 6.6540, VQ Loss: 3.2673, VQ Weight: 0.1000\n",
      "Epoch 4, Batch 15/21, Step 100, Loss: 6.9327, CTC Loss: 6.6038, VQ Loss: 3.2901, VQ Weight: 0.1000\n",
      "Epoch 5, Batch 14/21, Step 120, Loss: 6.9425, CTC Loss: 6.6108, VQ Loss: 3.3181, VQ Weight: 0.1000\n",
      "Epoch 6, Batch 13/21, Step 140, Loss: 6.9188, CTC Loss: 6.5838, VQ Loss: 3.3519, VQ Weight: 0.1000\n",
      "Epoch 7, Batch 12/21, Step 160, Loss: 6.9299, CTC Loss: 6.5913, VQ Loss: 3.3884, VQ Weight: 0.0999\n",
      "Epoch 8, Batch 11/21, Step 180, Loss: 6.9299, CTC Loss: 6.5870, VQ Loss: 3.4317, VQ Weight: 0.0999\n",
      "Epoch 9, Batch 10/21, Step 200, Loss: 6.9295, CTC Loss: 6.5817, VQ Loss: 3.4809, VQ Weight: 0.0999\n",
      "Epoch 10, Batch 9/21, Step 220, Loss: 6.9283, CTC Loss: 6.5751, VQ Loss: 3.5357, VQ Weight: 0.0999\n",
      "Epoch 11, Batch 8/21, Step 240, Loss: 6.9262, CTC Loss: 6.5670, VQ Loss: 3.5962, VQ Weight: 0.0999\n",
      "Epoch 12, Batch 7/21, Step 260, Loss: 6.9358, CTC Loss: 6.5697, VQ Loss: 3.6657, VQ Weight: 0.0999\n",
      "Epoch 13, Batch 6/21, Step 280, Loss: 6.9378, CTC Loss: 6.5645, VQ Loss: 3.7395, VQ Weight: 0.0998\n",
      "Epoch 14, Batch 5/21, Step 300, Loss: 6.9180, CTC Loss: 6.5366, VQ Loss: 3.8204, VQ Weight: 0.0998\n",
      "Epoch 15, Batch 4/21, Step 320, Loss: 6.9223, CTC Loss: 6.5322, VQ Loss: 3.9088, VQ Weight: 0.0998\n",
      "Epoch 16, Batch 3/21, Step 340, Loss: 6.9517, CTC Loss: 6.5521, VQ Loss: 4.0045, VQ Weight: 0.0998\n",
      "Epoch 17, Batch 2/21, Step 360, Loss: 6.9282, CTC Loss: 6.5180, VQ Loss: 4.1119, VQ Weight: 0.0997\n",
      "Epoch 18, Batch 1/21, Step 380, Loss: 6.9593, CTC Loss: 6.5379, VQ Loss: 4.2259, VQ Weight: 0.0997\n",
      "Epoch 19, Batch 0/21, Step 400, Loss: 6.9705, CTC Loss: 6.5370, VQ Loss: 4.3497, VQ Weight: 0.0997\n",
      "Epoch 19, Batch 20/21, Step 420, Loss: 6.9753, CTC Loss: 6.5285, VQ Loss: 4.4846, VQ Weight: 0.0996\n",
      "Epoch 20, Batch 19/21, Step 440, Loss: 6.9943, CTC Loss: 6.5329, VQ Loss: 4.6323, VQ Weight: 0.0996\n",
      "Epoch 21, Batch 18/21, Step 460, Loss: 6.9801, CTC Loss: 6.5031, VQ Loss: 4.7907, VQ Weight: 0.0996\n",
      "Epoch 22, Batch 17/21, Step 480, Loss: 6.9726, CTC Loss: 6.4790, VQ Loss: 4.9594, VQ Weight: 0.0995\n",
      "Epoch 23, Batch 16/21, Step 500, Loss: 7.0057, CTC Loss: 6.4942, VQ Loss: 5.1411, VQ Weight: 0.0995\n",
      "Model saved to models/test21/model_step_500.pth\n",
      "Epoch 24, Batch 15/21, Step 520, Loss: 7.0121, CTC Loss: 6.4815, VQ Loss: 5.3345, VQ Weight: 0.0995\n",
      "Epoch 25, Batch 14/21, Step 540, Loss: 7.0241, CTC Loss: 6.4726, VQ Loss: 5.5472, VQ Weight: 0.0994\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[12], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;18m__name__\u001b[39m \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m__main__\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m----> 2\u001b[0m     \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[10], line 112\u001b[0m, in \u001b[0;36mmain\u001b[1;34m()\u001b[0m\n\u001b[0;32m    109\u001b[0m     total_loss \u001b[38;5;241m=\u001b[39m ctc_loss\n\u001b[0;32m    111\u001b[0m total_loss \u001b[38;5;241m=\u001b[39m total_loss \u001b[38;5;241m/\u001b[39m gradient_accumulation_steps\n\u001b[1;32m--> 112\u001b[0m \u001b[43mtotal_loss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    114\u001b[0m ctc_losses\u001b[38;5;241m.\u001b[39mappend(ctc_loss\u001b[38;5;241m.\u001b[39mitem())\n\u001b[0;32m    115\u001b[0m vq_losses\u001b[38;5;241m.\u001b[39mappend(vq_loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m vq_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0.0\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Kamil\\Desktop\\Coding\\HuggingFace\\Agent\\lib\\site-packages\\torch\\_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    647\u001b[0m     )\n\u001b[1;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Kamil\\Desktop\\Coding\\HuggingFace\\Agent\\lib\\site-packages\\torch\\autograd\\__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[0;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Kamil\\Desktop\\Coding\\HuggingFace\\Agent\\lib\\site-packages\\torch\\autograd\\graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[1;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[0;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[0;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Variable\u001b[38;5;241m.\u001b[39m_execution_engine\u001b[38;5;241m.\u001b[39mrun_backward(  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    825\u001b[0m         t_outputs, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[0;32m    826\u001b[0m     )  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[0;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Agent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
